{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoders\n",
    "This is a shallow neural network to automatically perform feature engineering and feature selection. \n",
    "\n",
    "One other area that unsupervised learning excels in is *feature extraction*, which is a method used to generate a new feature representation from an original set of features. \n",
    "\n",
    "The new feature representation is called a *learned representation* and is used to improve performance on supervised learning problems. \n",
    "\n",
    "Autoencoders are one such form of feature extraction. They use **feed forward, non-recurrent neural network** to perform *representation learning*. \n",
    "\n",
    "In autoencoders which are a form of representation learning - each layer of the neural network learns a representation of the original features, and subsequent layers build on the representation learned by the preceding layers. \n",
    "\n",
    "Layer by layer, the autoencoder learns increasingly complicated representations from simpler ones, building what is known as hierarchy of concepts that become more and more abstract. \n",
    "\n",
    "The output layer is the final newly learned representation of the original features. This learned representation can then be used as input into a supervised learning model with the objective of improving the generalization error. \n",
    "\n",
    "# Neural Networks\n",
    "Neural Networks perfrom representation learning, where each layer of the neural network learns a representation from the previous layer. \n",
    "\n",
    "By building more nuanced and detailed representations layer by layer, neural networks can accomplish pretty amazing tasks such as computer vision, speech recogniion, and machine translation. \n",
    "\n",
    "The major difference between machine learning using neural networks and classical machine learning is that a lot of the feature representation is automatically performed in the neural networks case and is hand-designed in classical machine learning. \n",
    "\n",
    "Each node are fed into an activation function, which determines what value of the current layer is fed into the next layer of the neural network. Common activation functions: sigmoid, tanh, relu. The final activation function is usually the softmax function, which outputs a class probability that the input observation falls in.\n",
    "\n",
    "Neural networks may also have bias nodes; these nodes are always constant values and, unlike the normal nodes, are not connected to the previous layer. Rather, they allow the output of an activation function to be shifted lower or higher. With the hidden layers - including the nodes, bias nodes, and activation functions - the neural network is trying to learn the right function approximation to use to map the input layer to the output layer. \n",
    "\n",
    "In the case of supervised learning problems, this is pretty straight forward. The input layer represents the features that are fed into the neural network, and the output layer represents the label assigned to each observation. \n",
    "\n",
    "During the training process, the neural network determines which weights across the neural network help minimize the error between its predicted label for each observation and the true label. \n",
    "\n",
    "**unsupervised learning** problems, the neural network learns representations of the input layer via the various hidden layers but is not guided by labels. \n",
    "\n",
    "Neural networks are incredibly powerful and are capable of modeling complex nonlinear relationships to a degree that classical machine learning algorithms struggle with. \n",
    "\n",
    "There is a potential risk. Because neural networks can model complex nonlinear relationships, they are also much more prone to overfitting. Thus regularization of these networks is vital to their performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder: The Encoder and the Decoder\n",
    "An autoecoder has two parts: an encoder and decoder. \n",
    "\n",
    "The **encoder** converts the input set of features into a different representation. \n",
    "\n",
    "The **decoder** converts this newly learned representation to the original format. \n",
    "\n",
    "The core concept of an autoencoder is similar concept of dimensionality reduction. Similar to dimensionality reduction, an auto-encoder does not memorize the original observations and features, which would be an identify function. Rather, autoencoders must **approximate** the original observations as closely as possible - but not exactly - using a newly learned representation. \n",
    "\n",
    "In other words: The autoencoder learns an **approximation** of the identity function \n",
    "\n",
    "Since the autoencoder is contrained, it is forced to learn the most salient properties of the original data, capturing the underlying structure of the data; this is similar to what happens in dimensionality reduction.\n",
    "\n",
    "The constraint is very import attribute of autoencoders - that is the constraint forces the autoencoder to intelligently choose which import information to capture and which irrelevant or less import information to discard. \n",
    "\n",
    "## Undercomplete Autoencoders\n",
    "We care most about the **encoder** because this component is the one that learns a new representation of the original data. \n",
    "\n",
    "This new representation is the new set of features derived from the original set of features and observations. \n",
    "\n",
    "* Encoder = $h = f(x)$\n",
    "* Decoder = $r = g(h)$\n",
    "\n",
    "Therefor we will aim to build: \n",
    "\n",
    "## $$g(f(x)) \\approx x$$\n",
    "\n",
    "The term **autocomplete autoencoder** is a mechanism to constrain the encoder function's output $h$, to have fewer dimensions than $x$. \n",
    "\n",
    "Constrained in this manner, the autoencoder attempts to minimize a *loss function* we define such that the **reconstruction error** - after the decoder reconstructs the observations approximately using the encoder's output - is as small as possible. \n",
    "\n",
    "The decoder's last layer will contain as many layers as the encoders input, that is the reproduce a reconstructed output of the same dimensions as the raw input. \n",
    "\n",
    "**WHEN** the decoder is linear and the loss function is teh mean squared error, an undercoplete autoencoder learns the same sort of new representations as PCA. \n",
    "\n",
    "**HOWEVER**, if the encoder and decoder functions are nonlinear, the autoencoder can learn much more complex nonlinear representations. \n",
    "\n",
    "## Overcomplete Autoencoders\n",
    "If the encoder learns a representation in a greater numbre of dimensions than the original input dimensions, the autoencoder is considered overcomplete. \n",
    "\n",
    "If we employ some form of regularization, which penalizes the neural network for learning unecessarily complex functions, overcomplete autoencoders can be used successfully for dimensionality reduction and automatic feature engineering. \n",
    "\n",
    "Regularized overcomplete autoencoders are harder to design successfully but are potentially more powerful because they can learn more complex but not overly complex representations that better approximate the original observations without copying them precisely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense vs. Sparse Autoencoders\n",
    "Normal autoencoders output a dense final matrix such taht a handful of features have the most salient information that has been captured about the original data. \n",
    "\n",
    "HOWEVER, we may want to output a sparse final matrix such that the information captured is more well-distributed across the features that the autoencoder learns. \n",
    "\n",
    "To do this, we need to include not just a *reconstruction error* as part of the autoencoder but also a *sparsity penalty* so that the autoencoder must take the sparsity of the final matrix into consideration. \n",
    "\n",
    "Sparse autoencoders are generally overcomplete - the hidden layers have more units than the number of input features with the caveat that only a small fraction of the hidden units are allowed to be active at the same time. \n",
    "\n",
    "## Denoising Autoencoder\n",
    "In some cases, we may want the autoencoder we design to more aggressibely ignore the noise in the data, especially if we suspect the original data is corrupted to some degree. \n",
    "\n",
    "Examples may be: recording a conversation between two people at a noise coffee shop in the middle of the day. In which we would want to isolate the conversation (the signal) from the background chatter (noise). \n",
    "\n",
    "Or a dataset of images that are grainy or distorted due to low resolution or some blurring effect. We will want to isolate the core image from the distrotion. \n",
    "\n",
    "Therefor we design a *denoising autoencoder* that receives the corrupted data as input and is trained to output the original, uncorrupted data as best as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n",
    "An alternative autoencoder known as *variational autoencoder* has an encoder that outputs two vectors instead of one: a vector of means ```mu```, and a vector of standard deviations ```sigma```. \n",
    "\n",
    "These two vectors form random variables such taht the $ith$ element of ```mu``` and ```sigma``` correspond to the mean and standard deviation of the $ith$ random variable. \n",
    "\n",
    "By forming this stochastic output via its encoder, the variational autoencoder is able to sample across a continous space based on what it has learned from the input data \n",
    "\n",
    "The variational autoencoder is not confined to just the examples it has trained on but can generalize and output new examples even if it may have never seen precisely similar ones before. \n",
    "\n",
    "This is incredibly powerful because now the variational autoencoders can generate new synthetic data that appears to belong in the distribution the variational autoencoder has learned from the original input data. \n",
    "\n",
    "Such advances has led to an entirely new and trending field in unsupervised learning known as generative modeling: including *generative adversarial networks*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
