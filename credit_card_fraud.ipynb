{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Fraud Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegomedina-bernal/miniconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mlp\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing as pp\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, precision_recall_curve, average_precision_score, roc_curve, auc, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-10 01:14:33--  https://github.com/aapatel09/handson-unsupervised-learning/raw/master/datasets/credit_card_data/credit_card.csv\n",
      "Resolving github.com (github.com)... 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/aapatel09/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv [following]\n",
      "--2019-05-10 01:14:33--  https://media.githubusercontent.com/media/aapatel09/handson-unsupervised-learning/master/datasets/credit_card_data/credit_card.csv\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 199.232.36.133\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|199.232.36.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 150828752 (144M) [text/plain]\n",
      "Saving to: ‘credit_card.csv.1’\n",
      "\n",
      "credit_card.csv.1   100%[===================>] 143.84M  11.2MB/s    in 15s     \n",
      "\n",
      "2019-05-10 01:14:48 (9.30 MB/s) - ‘credit_card.csv.1’ saved [150828752/150828752]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "!wget https://github.com/aapatel09/handson-unsupervised-learning/raw/master/datasets/credit_card_data/credit_card.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "0,-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.338320769942518,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.311169353699879,1.46817697209427,-0.470400525259478,0.207971241929242,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.128539358273528,-0.189114843888824,0.133558376740387,-0.0210530534538215,149.62,\"0\"\n",
      "0,1.19185711131486,0.26615071205963,0.16648011335321,0.448154078460911,0.0600176492822243,-0.0823608088155687,-0.0788029833323113,0.0851016549148104,-0.255425128109186,-0.166974414004614,1.61272666105479,1.06523531137287,0.48909501589608,-0.143772296441519,0.635558093258208,0.463917041022171,-0.114804663102346,-0.183361270123994,-0.145783041325259,-0.0690831352230203,-0.225775248033138,-0.638671952771851,0.101288021253234,-0.339846475529127,0.167170404418143,0.125894532368176,-0.00898309914322813,0.0147241691924927,2.69,\"0\"\n"
     ]
    }
   ],
   "source": [
    "# Let's move it to the appriopriate file\n",
    "!mv credit_card.csv ./datasets/credit_card_data/credit_card_data.csv\n",
    "\n",
    "# check the head\n",
    "!head -n 3 ./datasets/credit_card_data/credit_card_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now load it into pandas\n",
    "data = pd.read_csv('./datasets/credit_card_data/credit_card_data.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V21',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V28',\n",
       " 'Amount',\n",
       " 'Class']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the classifications\n",
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "As shown above, there are only two classes. ```0``` reppresents normal while ```1``` represents some fraud. \n",
    "\n",
    "### Standardization\n",
    "We will then perform standardization which rescales the data to have a mean of zero and a standard deviation of one\n",
    "\n",
    "*some machine learning solutions are very sensitive to the scale of the data, so having all the data on the same relative scale via standardization is a good machine learning practice.* \n",
    "\n",
    "*another common method to scale data is **normalization**, which rescales the data to a zero to one range. Unlike standarized data, all the normalized data is on a positive scale* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NaNs\n",
    "data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287 ms ± 8.87 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 ms ± 4.16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "def uniquecount(el):\n",
    "    return len(el.unique())\n",
    "\n",
    "data.apply(uniquecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Feature Matrix and Label Arrays\n",
    "We will create and standardize the feature Matrix X and isolate the labels array y (1 = fraud, 0 = not fraud). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data.copy().drop(['Class'], axis='columns')\n",
    "data_y = data['Class'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9   ...         V20       V21       V22       V23  \\\n",
       "0  0.098698  0.363787   ...    0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654   ...    0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024   ...   -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739   ...    0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize the feature matrix X\n",
    "We will now rescale the feature matrix so that each feature, except for time, has a mean of zero and a standard deviation of one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing features we will scale\n",
    "features_to_scale = data_X.drop(['Time'], axis='columns').columns\n",
    "\n",
    "s_X  = pp.StandardScaler(copy=True)\n",
    "\n",
    "# Standarizing\n",
    "data_X.loc[:, features_to_scale] = s_X.fit_transform(data_X[features_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.694242</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>1.672773</td>\n",
       "      <td>0.973366</td>\n",
       "      <td>-0.245117</td>\n",
       "      <td>0.347068</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.082637</td>\n",
       "      <td>0.331128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326118</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.382854</td>\n",
       "      <td>-0.176911</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>0.246585</td>\n",
       "      <td>-0.392170</td>\n",
       "      <td>0.330892</td>\n",
       "      <td>-0.063781</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608496</td>\n",
       "      <td>0.161176</td>\n",
       "      <td>0.109797</td>\n",
       "      <td>0.316523</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>-0.061820</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>-0.232494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089611</td>\n",
       "      <td>-0.307377</td>\n",
       "      <td>-0.880077</td>\n",
       "      <td>0.162201</td>\n",
       "      <td>-0.561131</td>\n",
       "      <td>0.320694</td>\n",
       "      <td>0.261069</td>\n",
       "      <td>-0.022256</td>\n",
       "      <td>0.044608</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.693500</td>\n",
       "      <td>-0.811578</td>\n",
       "      <td>1.169468</td>\n",
       "      <td>0.268231</td>\n",
       "      <td>-0.364572</td>\n",
       "      <td>1.351454</td>\n",
       "      <td>0.639776</td>\n",
       "      <td>0.207373</td>\n",
       "      <td>-1.378675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680975</td>\n",
       "      <td>0.337632</td>\n",
       "      <td>1.063358</td>\n",
       "      <td>1.456320</td>\n",
       "      <td>-1.138092</td>\n",
       "      <td>-0.628537</td>\n",
       "      <td>-0.288447</td>\n",
       "      <td>-0.137137</td>\n",
       "      <td>-0.181021</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.493325</td>\n",
       "      <td>-0.112169</td>\n",
       "      <td>1.182516</td>\n",
       "      <td>-0.609727</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>0.936150</td>\n",
       "      <td>0.192071</td>\n",
       "      <td>0.316018</td>\n",
       "      <td>-1.262503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269855</td>\n",
       "      <td>-0.147443</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>-0.304777</td>\n",
       "      <td>-1.941027</td>\n",
       "      <td>1.241904</td>\n",
       "      <td>-0.460217</td>\n",
       "      <td>0.155396</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.591330</td>\n",
       "      <td>0.531541</td>\n",
       "      <td>1.021412</td>\n",
       "      <td>0.284655</td>\n",
       "      <td>-0.295015</td>\n",
       "      <td>0.071999</td>\n",
       "      <td>0.479302</td>\n",
       "      <td>-0.226510</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529939</td>\n",
       "      <td>-0.012839</td>\n",
       "      <td>1.100011</td>\n",
       "      <td>-0.220123</td>\n",
       "      <td>0.233250</td>\n",
       "      <td>-0.395202</td>\n",
       "      <td>1.041611</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.651816</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -0.694242 -0.044075  1.672773  0.973366 -0.245117  0.347068  0.193679   \n",
       "1   0.0  0.608496  0.161176  0.109797  0.316523  0.043483 -0.061820 -0.063700   \n",
       "2   1.0 -0.693500 -0.811578  1.169468  0.268231 -0.364572  1.351454  0.639776   \n",
       "3   1.0 -0.493325 -0.112169  1.182516 -0.609727 -0.007469  0.936150  0.192071   \n",
       "4   2.0 -0.591330  0.531541  1.021412  0.284655 -0.295015  0.071999  0.479302   \n",
       "\n",
       "         V8        V9    ...          V20       V21       V22       V23  \\\n",
       "0  0.082637  0.331128    ...     0.326118 -0.024923  0.382854 -0.176911   \n",
       "1  0.071253 -0.232494    ...    -0.089611 -0.307377 -0.880077  0.162201   \n",
       "2  0.207373 -1.378675    ...     0.680975  0.337632  1.063358  1.456320   \n",
       "3  0.316018 -1.262503    ...    -0.269855 -0.147443  0.007267 -0.304777   \n",
       "4 -0.226510  0.744326    ...     0.529939 -0.012839  1.100011 -0.220123   \n",
       "\n",
       "        V24       V25       V26       V27       V28    Amount  \n",
       "0  0.110507  0.246585 -0.392170  0.330892 -0.063781  0.244964  \n",
       "1 -0.561131  0.320694  0.261069 -0.022256  0.044608 -0.342475  \n",
       "2 -1.138092 -0.628537 -0.288447 -0.137137 -0.181021  1.160686  \n",
       "3 -1.941027  1.241904 -0.460217  0.155396  0.186189  0.140534  \n",
       "4  0.233250 -0.395202  1.041611  0.543620  0.651816 -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Feature Selection\n",
    "Feature Engineering involves creating new features, for example: calculating ratios or counts or sums from the original feature to help the machine learning algorithm extract a stronger signal from the dataset.\n",
    "\n",
    "Feature selection involves selecting a subset of features for training, effectively removing some of the less relevant features from consideraton. This may help prevent the machine learning algorithm from overfitting to the noise in the dataset.\n",
    "\n",
    "In our dataset, PCA has been performed on all features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prep\n",
    "We need to split the data into a training and a test set, select a cost function, and prepare for k-fold cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.33, random_state=2018, stratify=data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190820, 30)\n",
      "(93987, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Cost Function\n",
    "Before we train on the training set, we need a cost function also referred to as the error rate or value function to pass into the machine learning algorithm. \n",
    "\n",
    "The machine learning algorithm will try to minimize this cost function by learning from training examples.\n",
    "\n",
    "Given there are two classes and this is supervised learning, we will pick binary classification log loss. \n",
    "\n",
    "Which will calculate cross-entropy between the true labels and the model-based predictions. \n",
    "\n",
    "The closer the fraud probabilities are to the true labels, the lower the value of the log loss function. \n",
    "\n",
    "## Create k-Fold Cross-Validation State\n",
    "We will further split the training data into both test and validation set to further evaluate and measure generalization better. \n",
    "\n",
    "In our case we are splitting our data into 5, therefor 1/5 will be validation while 4/5 will be training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n",
    "k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression\n",
    "We start with a basic classification algorithm. \n",
    "\n",
    "The stronger the regularization, the greater the penalty the machine learning algorithm applies to complexity. Therefor regularization nudges the machine learning algorithm to prefer simpler models to more complex ones.\n",
    "\n",
    "Smaller value of ```C``` is stronger regularization. \n",
    "\n",
    "```class_weight='balanced'``` signals to the logistic regression algorithm that we have an imbalanced class problem (too few fraud examples), it will then weight the positive labels more heavily as it trains. \n",
    "\n",
    "The weights will be inversely proportional to the class fequencies; the algorithm will assign higher weights to the rare positive labels and lower weights to the more frequent negative labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "penalty = 'l2'                # regularization\n",
    "C = 1.0                       # regularization strength\n",
    "class_weight = 'balanced'\n",
    "random_state = 2018\n",
    "solver = 'liblinear'\n",
    "n_jobs = 1\n",
    "\n",
    "log_regression = LogisticRegression(penalty=penalty, C=C, class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log loss: 0.10312126363723864\n",
      "CV log loss: 0.10891424425385518\n",
      "Training log loss: 0.10808547228589843\n",
      "CV log loss: 0.10349703681980418\n",
      "Training log loss: 0.09345906662732105\n",
      "CV log loss: 0.09507221692204695\n",
      "Training log loss: 0.11765487752642477\n",
      "CV log loss: 0.11832556578829838\n",
      "Training log loss: 0.12369327017158406\n",
      "CV log loss: 0.12197629101604389\n",
      "Logistic Regression Log Loss: 0.10955707096000972\n"
     ]
    }
   ],
   "source": [
    "training_score = []\n",
    "cv_scores = []\n",
    "predictions_based_on_k_folds = pd.DataFrame(data=[], index=y_train.index, columns=[0,1])\n",
    "\n",
    "model = log_regression\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), y_train.ravel()):\n",
    "    \n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n",
    "        X_train.iloc[cv_index,:]\n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n",
    "        y_train.iloc[cv_index]\n",
    "    \n",
    "    # Fitting model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Log loss\n",
    "    log_loss_training = log_loss(y_train_fold, model.predict_proba(X_train_fold)[:, 1])\n",
    "    \n",
    "    training_score.append(log_loss_training)\n",
    "    \n",
    "    predictions_based_on_k_folds.loc[X_cv_fold.index, :] = model.predict_proba(X_cv_fold)\n",
    "    \n",
    "    log_loss_cv = log_loss(y_cv_fold, predictions_based_on_k_folds.loc[X_cv_fold.index, 1])\n",
    "    \n",
    "    cv_scores.append(log_loss_cv)\n",
    "    \n",
    "    print(f'Training log loss: {log_loss_training}')\n",
    "    print(f'CV log loss: {log_loss_cv}')\n",
    "    \n",
    "log_loss_logistic_regression = log_loss(y_train, predictions_based_on_k_folds.loc[:,1])\n",
    "\n",
    "print(f'Logistic Regression Log Loss: {log_loss_logistic_regression}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision & Recall\n",
    "Now we will further evaluate and inspect our first model with plotting our **precision recall curve**. This is to measure just how precise our model is at making predictions. Precision is the number of true positives over the number of total positive predictions: \n",
    "\n",
    "In our case, how many of the fraud transactions did our model catch? \n",
    "\n",
    "A high precision means that of all our positive predictions, many are true positives (having a low false positive rate). \n",
    "\n",
    "Recall is the number of true positives over the number of total actual positives in the dataset. In other words, how many of the fraud transactions does the model catch?\n",
    "\n",
    "IF our solution has high precision but low recall, there would be a very small number of fraudulent transactions found but most would be truly fraudulent. \n",
    "\n",
    "IF our solution has low precision but high recall it would flag many of the transactions as fraudulent, thus catching a lot of fraud, but most of the flagged transactions would not be fraudulent.\n",
    "\n",
    "\n",
    "We must find high precision and high recall. \n",
    "\n",
    "A graph of the trade-off between precision and recall is known as the precision-recall curve. \n",
    "\n",
    "The higher the average precision, the better the solution\n",
    "\n",
    "## Receiver Operating Characteristic\n",
    "Another good evaluation metric is the area under the receiver operating characteristic. The receiver operating characteristic curve plots the true positive rate on the Y axis and the false positive rate on the X axis.\n",
    "\n",
    "The true positive rate can also be referred to as the sensitivity, and the false positive rate can also be referred to as the 1-specificity.\n",
    "\n",
    "The closer the curve is to the top-left corner of the plot, the better the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Precision Recall Curve: Average Precision = 0.7200383065659999')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the precision recall curve to understand how precise our model is \n",
    "preds = pd.concat([y_train, predictions_based_on_k_folds.loc[:,1]], axis=1)\n",
    "preds.columns = ['true_label', 'prediction']\n",
    "\n",
    "predictions_based_on_k_folds_logistic_regression = preds.copy()\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(preds['true_label'], preds['prediction'])\n",
    "\n",
    "average_precision = average_precision_score(preds['true_label'], preds['prediction'])\n",
    "\n",
    "# Plotting\n",
    "plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.3)\n",
    "\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "plt.title(f'Precision Recall Curve: Average Precision = {average_precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this model, we have an average PRC, with an average precision of 72%. That is, at 80% recall, we have a 72% precision. \n",
    "\n",
    "This basically means, 72% were truly fraudulent (what the model flagged), while 28% were incorrectly flagged as fraudulent.\n",
    "\n",
    "We will no go onto measure ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(preds['true_label'], preds['prediction'])\n",
    "\n",
    "area_under_ROC = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
    "plt.plot([0,1], [0,1], color='k', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'Receiver operating characteristic: Area under the curve = {area_under_ROC}')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random Forests\n",
    "We will follow the same work-flow but now introduced another model: Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "n_estimators = 10\n",
    "max_features = 'auto'\n",
    "max_depth = None\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "min_weight_fraction_leaf = 0.0\n",
    "max_leaf_nodes = None\n",
    "bootstrap = True\n",
    "oob_score = False\n",
    "n_jobs = -1\n",
    "random_state = 2018\n",
    "class_weight = 'balanced'\n",
    "\n",
    "RFC = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Log Loss: 0.00038666468324144877\n",
      "CV Log Loss: 0.0153965105968685\n",
      "\n",
      "Training Log Loss: 0.0004263217817373472\n",
      "CV Log Loss: 0.010755953739346607\n",
      "\n",
      "Training Log Loss: 0.0004088335711949752\n",
      "CV Log Loss: 0.015386056391658222\n",
      "\n",
      "Training Log Loss: 0.000430335389253032\n",
      "CV Log Loss: 0.0047639585692903445\n",
      "\n",
      "Training Log Loss: 0.00045596089565145143\n",
      "CV Log Loss: 0.005619921221138471\n",
      "\n",
      "Random Forest Log Loss: 0.010384480103660427\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "training_scores = []\n",
    "cv_scores = []\n",
    "predictions_based_on_k_folds = pd.DataFrame(data=[], index=y_train.index, columns=[0,1])\n",
    "\n",
    "model = RFC\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n",
    "    \n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n",
    "    \n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    log_loss_training = log_loss(y_train_fold, model.predict_proba(X_train_fold))\n",
    "    \n",
    "    training_scores.append(log_loss_training)\n",
    "    \n",
    "    predictions_based_on_k_folds.loc[X_cv_fold.index,:] = model.predict_proba(X_cv_fold)\n",
    "    \n",
    "    log_loss_cv = log_loss(y_cv_fold, predictions_based_on_k_folds.loc[X_cv_fold.index,1])\n",
    "    \n",
    "    cv_scores.append(log_loss_cv)\n",
    "    \n",
    "    print(f'Training Log Loss: {log_loss_training}')\n",
    "    print(f'CV Log Loss: {log_loss_cv}\\n')\n",
    "    \n",
    "log_loss_random_forest_classifier = log_loss(y_train, predictions_based_on_k_folds.loc[:,1])\n",
    "\n",
    "print(f'Random Forest Log Loss: {log_loss_random_forest_classifier}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Gradient Boosting Machine (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "params_XGB = {\n",
    "    'nthread':16,\n",
    "    'learning rate':0.3,\n",
    "    'gamma':0,\n",
    "    'max_depth':6,\n",
    "    'min_child_weight':1,\n",
    "    'max_delta_step':0,\n",
    "    'subsample':1.0,\n",
    "    'colsample_bytree':1.0,\n",
    "    'objective':'binary:logistic',\n",
    "    'num_class':1,\n",
    "    'eval_metric':'logloss',\n",
    "    'seed':2018,\n",
    "    'silent':1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "training_scores = []\n",
    "cv_scores = []\n",
    "predictions_based_on_k_folds = pd.DataFrame(data=[], index=y_train.index, columns=[0,1])\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n",
    "    \n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n",
    "    \n",
    "    d_train = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n",
    "    d_cv = xgb.DMatrix(data=X_cv_fold)\n",
    "    \n",
    "    bst = xgb.cv(params_XGB, d_train, num_boost_round=2000, nfold=5, early_stopping_rounds=200, verbose_eval=50)\n",
    "    \n",
    "    best_rounds = np.argmin(bst['test-logloss-mean'])\n",
    "    bst = xgb.train(params_XGB, d_train, best_rounds)\n",
    "    \n",
    "    log_loss_training = log_loss(y_train_fold, bst.predict(d_train))\n",
    "    training_scores.append(log_loss_training)\n",
    "    \n",
    "    predictions_based_on_k_folds.loc[X_cv_fold.index, 'prediction'] = bst.predict(d_cv)\n",
    "    \n",
    "    log_loss_cv = log_loss(y_cv_fold, predictions_based_on_k_folds.loc[X_cv_fold.index,'prediction'])\n",
    "    \n",
    "    cv_scores.append(log_loss_cv)\n",
    "    \n",
    "    print(f'Training Log Loss: {log_loss_training}')\n",
    "    print(f'CV Log Loss: {log_loss_cv}\\n')\n",
    "    \n",
    "log_loss_xgboost_gradient_boosting = log_loss(y_train, predictions_based_on_k_folds.loc[:,'prediction'])\n",
    "\n",
    "print(f'XGBoost Gradient Boosting Log Loss: {log_loss_xgboost_gradient_boosting}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: Gradient Boosting Machine (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_light_GB = {\n",
    "    'task': 'train',\n",
    "    'application': 'binary',\n",
    "    'num_class': 1,\n",
    "    'boosting': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'metric_freq': 50,\n",
    "    'is_training_metric': False,\n",
    "    'max_depth': 4,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 1.0,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 0,\n",
    "    'bagging_seed': 2018,\n",
    "    'verbose': 0,\n",
    "    'num_threads': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "training_scores = []\n",
    "cv_scores = []\n",
    "predictions_based_on_k_folds = pd.DataFrame(data=[], index=y_train.index, columns=['predictions'])\n",
    "\n",
    "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), y_train.ravel()):\n",
    "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]\n",
    "    \n",
    "    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
    "    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
